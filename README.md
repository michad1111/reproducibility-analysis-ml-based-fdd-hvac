# Reproducibility of Machine Learning-Based Fault Detection and Diagnosis for HVAC Systems in Buildings: An Empirical Study

## Abstract

Clear and well-documented research experiments are essential, as they allow researchers to reproduce results and verify the authors' claims. Reproducibility is an inherent characteristic of research and should be given heightened importance; however, the rise of machine learning (ML) techniques has introduced new challenges to achieving it. Thus, both the experimental configuration and the proposed methodology should be documented in a clear and standardized manner. Recently, the use of ML techniques has grown common in building systems scientific disciplines, similar to other fields. As this discipline expands in scope and complexity of its methods, concerns have arisen—much like in other fields—about the materials shared along with the proper documentation to support reproducibility. Therefore, in this study, we aim to analyze ML-based fault detection and diagnosis techniques for building systems and quantify them across a list of reproducibility dimensions. Our analysis revealed concerning findings, indicating that nearly all articles are technically irreproducible due to insufficient disclosure across reproducibility dimensions, i.e., *data*, *preprocessing*, *evaluation*, and especially *hyperparameter* and *code availability*. These findings highlight the need for intervention and reproducibility guidelines. With this, we aim to contribute to the development of more transparent, reliable, and verifiable research in the field.

## Contents

- `data/` — Directory contains recoreded reproducibility checklist.
- `results/` — Output of the reproducibility assessment (e.g., plots).
- `paper/` — The manuscript and supplementary materials.

